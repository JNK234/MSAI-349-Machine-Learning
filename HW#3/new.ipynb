{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 Manual Gradients and Updates (3.0 points): \n",
    "\n",
    "Repeat either Step 1 or Step 2 without using a deep\n",
    "learning platform. You can use symbolic differentiation tools like WolramAlpha, Mathematica,\n",
    "etc., to compute gradients. You can also calculate the gradients by hand. You may want to calculate,\n",
    "code and verify gradients for individual components of your model and use the chain rule to build\n",
    "the gradients for specific weights and biases. You may also want to consider using for loops\n",
    "instead of matrix algebra in some parts of your code to avoid the ambiguity of broadcasting. You\n",
    "may find it helpful to “calibrate” intermediate quantities in your implementation against your\n",
    "PyTorch implementation from Step 1 or 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility when debugging \n",
    "def set_random_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "set_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_datasets():\n",
    "    datasets = {}\n",
    "    dataset_names = ['center_surround', 'spiral', 'two_gaussians', 'xor']\n",
    "    \n",
    "    for name in dataset_names:\n",
    "        train_data = pd.read_csv(f'{name}_train.csv')\n",
    "        val_data = pd.read_csv(f'{name}_valid.csv')\n",
    "        test_data = pd.read_csv(f'{name}_test.csv')\n",
    "        \n",
    "        X_train = train_data[['x1', 'x2']].values\n",
    "        y_train = train_data['label'].values\n",
    "        X_val = val_data[['x1', 'x2']].values\n",
    "        y_val = val_data['label'].values\n",
    "        X_test = test_data[['x1', 'x2']].values\n",
    "        y_test = test_data['label'].values\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        datasets[name] = {\n",
    "            'train': (X_train, y_train),\n",
    "            'valid': (X_val, y_val),\n",
    "            'test': (X_test, y_test)\n",
    "        }\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class ManualNN:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        self.W1 = np.random.randn((input_dim, hidden_dim))\n",
    "        self.b1 = np.random.randn((hidden_dim, 1))\n",
    "        self.W2 = np.random.randn((hidden_dim, output_dim))\n",
    "        self.b2 = np.random.randn(())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hitech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
